{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "sys.path.append('/home/damon/Documents/')\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import mmdet\n",
    "import mmcv\n",
    "import deformable as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch, net):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx % 128 == 0:\n",
    "            print('train Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch, net):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx  % 128 == 0:\n",
    "                print('test Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                    % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dconv = D.DeformConv2d(3, 32, kernel_size=3, stride=1, padding=1).cuda()\n",
    "conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_op = nn.Conv2d\n",
    "conv_op = D.DeformConv2d\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = conv_op(3, 32, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = conv_op(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = conv_op(64, 128, 3, stride=2, padding=1)\n",
    "        self.conv4 = conv_op(128, 256, 3, stride=2, padding=1)\n",
    "        self.conv1x1 = nn.Conv2d(256, 512, 1, stride=1, padding=0)\n",
    "        self.fc = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train Loss: 2.301 | Acc: 9.375% (12/128)\n",
      "train Loss: 2.298 | Acc: 13.608% (2247/16512)\n",
      "train Loss: 2.294 | Acc: 14.449% (4753/32896)\n",
      "train Loss: 2.287 | Acc: 15.158% (7470/49280)\n",
      "test Loss: 2.237 | Acc: 21.000% (21/100)\n",
      "\n",
      "Epoch: 1\n",
      "train Loss: 2.256 | Acc: 18.750% (24/128)\n",
      "train Loss: 2.236 | Acc: 19.265% (3181/16512)\n",
      "train Loss: 2.205 | Acc: 20.088% (6608/32896)\n",
      "train Loss: 2.176 | Acc: 20.980% (10339/49280)\n",
      "test Loss: 2.008 | Acc: 35.000% (35/100)\n",
      "\n",
      "Epoch: 2\n",
      "train Loss: 2.104 | Acc: 25.781% (33/128)\n",
      "train Loss: 2.076 | Acc: 24.673% (4074/16512)\n",
      "train Loss: 2.045 | Acc: 26.246% (8634/32896)\n",
      "train Loss: 2.011 | Acc: 27.524% (13564/49280)\n",
      "test Loss: 1.800 | Acc: 40.000% (40/100)\n",
      "\n",
      "Epoch: 3\n",
      "train Loss: 1.968 | Acc: 25.781% (33/128)\n",
      "train Loss: 1.889 | Acc: 31.256% (5161/16512)\n",
      "train Loss: 1.859 | Acc: 32.116% (10565/32896)\n",
      "train Loss: 1.840 | Acc: 32.946% (16236/49280)\n",
      "test Loss: 1.736 | Acc: 44.000% (44/100)\n",
      "\n",
      "Epoch: 4\n",
      "train Loss: 1.735 | Acc: 39.062% (50/128)\n",
      "train Loss: 1.751 | Acc: 35.913% (5930/16512)\n",
      "train Loss: 1.740 | Acc: 36.077% (11868/32896)\n",
      "train Loss: 1.722 | Acc: 36.757% (18114/49280)\n",
      "test Loss: 1.579 | Acc: 40.000% (40/100)\n",
      "\n",
      "Epoch: 5\n",
      "train Loss: 1.595 | Acc: 40.625% (52/128)\n",
      "train Loss: 1.655 | Acc: 38.657% (6383/16512)\n",
      "train Loss: 1.643 | Acc: 39.172% (12886/32896)\n",
      "train Loss: 1.635 | Acc: 39.554% (19492/49280)\n",
      "test Loss: 1.557 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 6\n",
      "train Loss: 1.540 | Acc: 50.781% (65/128)\n",
      "train Loss: 1.600 | Acc: 41.031% (6775/16512)\n",
      "train Loss: 1.587 | Acc: 41.464% (13640/32896)\n",
      "train Loss: 1.581 | Acc: 41.780% (20589/49280)\n",
      "test Loss: 1.490 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 7\n",
      "train Loss: 1.646 | Acc: 39.062% (50/128)\n",
      "train Loss: 1.528 | Acc: 43.381% (7163/16512)\n",
      "train Loss: 1.534 | Acc: 43.373% (14268/32896)\n",
      "train Loss: 1.530 | Acc: 43.519% (21446/49280)\n",
      "test Loss: 1.423 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 8\n",
      "train Loss: 1.637 | Acc: 37.500% (48/128)\n",
      "train Loss: 1.509 | Acc: 44.440% (7338/16512)\n",
      "train Loss: 1.500 | Acc: 44.896% (14769/32896)\n",
      "train Loss: 1.491 | Acc: 45.110% (22230/49280)\n",
      "test Loss: 1.387 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 9\n",
      "train Loss: 1.485 | Acc: 49.219% (63/128)\n",
      "train Loss: 1.471 | Acc: 45.621% (7533/16512)\n",
      "train Loss: 1.458 | Acc: 46.261% (15218/32896)\n",
      "train Loss: 1.452 | Acc: 46.617% (22973/49280)\n",
      "test Loss: 1.346 | Acc: 55.000% (55/100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3b1d9d76f91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-764966f1372b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, net)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-31d85a0f1f97>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deformable.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# (b, 2N, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# (b, h, w, 2N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deformable.py\u001b[0m in \u001b[0;36m_get_p\u001b[0;34m(self, offset, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# (1, 2N, 1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_p_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# (1, 2N, h, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mp_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_p_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deformable.py\u001b[0m in \u001b[0;36m_get_p_n\u001b[0;34m(self, N, dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# (2N, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "for epoch in range(start_epoch, start_epoch+50):\n",
    "    train(epoch, net)\n",
    "    test(epoch, net)\n",
    "print(time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train Loss: 2.304 | Acc: 8.594% (11/128)\n",
      "train Loss: 2.297 | Acc: 10.338% (1707/16512)\n",
      "train Loss: 2.289 | Acc: 12.415% (4084/32896)\n",
      "train Loss: 2.275 | Acc: 12.977% (6395/49280)\n",
      "test Loss: 2.187 | Acc: 20.000% (20/100)\n",
      "\n",
      "Epoch: 1\n",
      "train Loss: 2.193 | Acc: 17.969% (23/128)\n",
      "train Loss: 2.203 | Acc: 18.193% (3004/16512)\n",
      "train Loss: 2.176 | Acc: 19.972% (6570/32896)\n",
      "train Loss: 2.150 | Acc: 21.571% (10630/49280)\n",
      "test Loss: 2.025 | Acc: 34.000% (34/100)\n",
      "\n",
      "Epoch: 2\n",
      "train Loss: 2.070 | Acc: 25.781% (33/128)\n",
      "train Loss: 2.048 | Acc: 25.987% (4291/16512)\n",
      "train Loss: 2.027 | Acc: 26.553% (8735/32896)\n",
      "train Loss: 2.006 | Acc: 27.108% (13359/49280)\n",
      "test Loss: 1.894 | Acc: 38.000% (38/100)\n",
      "\n",
      "Epoch: 3\n",
      "train Loss: 1.961 | Acc: 29.688% (38/128)\n",
      "train Loss: 1.930 | Acc: 29.784% (4918/16512)\n",
      "train Loss: 1.910 | Acc: 30.356% (9986/32896)\n",
      "train Loss: 1.893 | Acc: 30.773% (15165/49280)\n",
      "test Loss: 1.776 | Acc: 41.000% (41/100)\n",
      "\n",
      "Epoch: 4\n",
      "train Loss: 1.770 | Acc: 36.719% (47/128)\n",
      "train Loss: 1.817 | Acc: 33.454% (5524/16512)\n",
      "train Loss: 1.790 | Acc: 34.259% (11270/32896)\n",
      "train Loss: 1.778 | Acc: 34.894% (17196/49280)\n",
      "test Loss: 1.676 | Acc: 44.000% (44/100)\n",
      "\n",
      "Epoch: 5\n",
      "train Loss: 1.771 | Acc: 33.594% (43/128)\n",
      "train Loss: 1.713 | Acc: 37.803% (6242/16512)\n",
      "train Loss: 1.702 | Acc: 37.825% (12443/32896)\n",
      "train Loss: 1.690 | Acc: 38.141% (18796/49280)\n",
      "test Loss: 1.565 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 6\n",
      "train Loss: 1.674 | Acc: 41.406% (53/128)\n",
      "train Loss: 1.642 | Acc: 39.886% (6586/16512)\n",
      "train Loss: 1.642 | Acc: 40.263% (13245/32896)\n",
      "train Loss: 1.630 | Acc: 40.666% (20040/49280)\n",
      "test Loss: 1.538 | Acc: 49.000% (49/100)\n",
      "\n",
      "Epoch: 7\n",
      "train Loss: 1.466 | Acc: 47.656% (61/128)\n",
      "train Loss: 1.598 | Acc: 41.400% (6836/16512)\n",
      "train Loss: 1.585 | Acc: 41.944% (13798/32896)\n",
      "train Loss: 1.578 | Acc: 42.248% (20820/49280)\n",
      "test Loss: 1.456 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 8\n",
      "train Loss: 1.720 | Acc: 39.062% (50/128)\n",
      "train Loss: 1.542 | Acc: 43.938% (7255/16512)\n",
      "train Loss: 1.542 | Acc: 43.701% (14376/32896)\n",
      "train Loss: 1.534 | Acc: 43.870% (21619/49280)\n",
      "test Loss: 1.411 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 9\n",
      "train Loss: 1.583 | Acc: 37.500% (48/128)\n",
      "train Loss: 1.506 | Acc: 45.216% (7466/16512)\n",
      "train Loss: 1.506 | Acc: 45.282% (14896/32896)\n",
      "train Loss: 1.500 | Acc: 45.455% (22400/49280)\n",
      "test Loss: 1.414 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 10\n",
      "train Loss: 1.571 | Acc: 42.969% (55/128)\n",
      "train Loss: 1.463 | Acc: 46.621% (7698/16512)\n",
      "train Loss: 1.462 | Acc: 46.793% (15393/32896)\n",
      "train Loss: 1.460 | Acc: 46.948% (23136/49280)\n",
      "test Loss: 1.353 | Acc: 53.000% (53/100)\n",
      "\n",
      "Epoch: 11\n",
      "train Loss: 1.362 | Acc: 47.656% (61/128)\n",
      "train Loss: 1.432 | Acc: 47.905% (7910/16512)\n",
      "train Loss: 1.432 | Acc: 47.915% (15762/32896)\n",
      "train Loss: 1.433 | Acc: 48.133% (23720/49280)\n",
      "test Loss: 1.270 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 12\n",
      "train Loss: 1.320 | Acc: 46.875% (60/128)\n",
      "train Loss: 1.428 | Acc: 48.450% (8000/16512)\n",
      "train Loss: 1.420 | Acc: 48.745% (16035/32896)\n",
      "train Loss: 1.412 | Acc: 48.803% (24050/49280)\n",
      "test Loss: 1.284 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 13\n",
      "train Loss: 1.324 | Acc: 44.531% (57/128)\n",
      "train Loss: 1.401 | Acc: 49.122% (8111/16512)\n",
      "train Loss: 1.393 | Acc: 49.441% (16264/32896)\n",
      "train Loss: 1.390 | Acc: 49.578% (24432/49280)\n",
      "test Loss: 1.316 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 14\n",
      "train Loss: 1.415 | Acc: 46.875% (60/128)\n",
      "train Loss: 1.369 | Acc: 50.678% (8368/16512)\n",
      "train Loss: 1.375 | Acc: 50.526% (16621/32896)\n",
      "train Loss: 1.371 | Acc: 50.588% (24930/49280)\n",
      "test Loss: 1.254 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 15\n",
      "train Loss: 1.315 | Acc: 57.031% (73/128)\n",
      "train Loss: 1.358 | Acc: 51.060% (8431/16512)\n",
      "train Loss: 1.351 | Acc: 51.228% (16852/32896)\n",
      "train Loss: 1.350 | Acc: 51.301% (25281/49280)\n",
      "test Loss: 1.235 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 16\n",
      "train Loss: 1.201 | Acc: 60.156% (77/128)\n",
      "train Loss: 1.354 | Acc: 51.738% (8543/16512)\n",
      "train Loss: 1.345 | Acc: 51.760% (17027/32896)\n",
      "train Loss: 1.337 | Acc: 52.007% (25629/49280)\n",
      "test Loss: 1.194 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 17\n",
      "train Loss: 1.328 | Acc: 54.688% (70/128)\n",
      "train Loss: 1.326 | Acc: 52.889% (8733/16512)\n",
      "train Loss: 1.320 | Acc: 52.672% (17327/32896)\n",
      "train Loss: 1.317 | Acc: 52.782% (26011/49280)\n",
      "test Loss: 1.168 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 18\n",
      "train Loss: 1.392 | Acc: 46.875% (60/128)\n",
      "train Loss: 1.306 | Acc: 53.022% (8755/16512)\n",
      "train Loss: 1.303 | Acc: 53.192% (17498/32896)\n",
      "train Loss: 1.300 | Acc: 53.450% (26340/49280)\n",
      "test Loss: 1.140 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 19\n",
      "train Loss: 1.354 | Acc: 46.094% (59/128)\n",
      "train Loss: 1.290 | Acc: 53.852% (8892/16512)\n",
      "train Loss: 1.292 | Acc: 53.794% (17696/32896)\n",
      "train Loss: 1.289 | Acc: 53.896% (26560/49280)\n",
      "test Loss: 1.129 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 20\n",
      "train Loss: 1.387 | Acc: 50.781% (65/128)\n",
      "train Loss: 1.273 | Acc: 54.233% (8955/16512)\n",
      "train Loss: 1.278 | Acc: 54.253% (17847/32896)\n",
      "train Loss: 1.276 | Acc: 54.239% (26729/49280)\n",
      "test Loss: 1.115 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 21\n",
      "train Loss: 1.367 | Acc: 54.688% (70/128)\n",
      "train Loss: 1.261 | Acc: 54.924% (9069/16512)\n",
      "train Loss: 1.263 | Acc: 54.742% (18008/32896)\n",
      "train Loss: 1.264 | Acc: 54.612% (26913/49280)\n",
      "test Loss: 1.131 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 22\n",
      "train Loss: 1.091 | Acc: 60.938% (78/128)\n",
      "train Loss: 1.237 | Acc: 55.959% (9240/16512)\n",
      "train Loss: 1.252 | Acc: 55.305% (18193/32896)\n",
      "train Loss: 1.251 | Acc: 55.408% (27305/49280)\n",
      "test Loss: 1.130 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 23\n",
      "train Loss: 1.327 | Acc: 50.000% (64/128)\n",
      "train Loss: 1.236 | Acc: 56.038% (9253/16512)\n",
      "train Loss: 1.237 | Acc: 56.016% (18427/32896)\n",
      "train Loss: 1.240 | Acc: 55.860% (27528/49280)\n",
      "test Loss: 1.115 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 24\n",
      "train Loss: 1.228 | Acc: 63.281% (81/128)\n",
      "train Loss: 1.214 | Acc: 56.298% (9296/16512)\n",
      "train Loss: 1.226 | Acc: 56.122% (18462/32896)\n",
      "train Loss: 1.224 | Acc: 56.252% (27721/49280)\n",
      "test Loss: 1.096 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 25\n",
      "train Loss: 1.316 | Acc: 55.469% (71/128)\n",
      "train Loss: 1.214 | Acc: 56.165% (9274/16512)\n",
      "train Loss: 1.216 | Acc: 56.286% (18516/32896)\n",
      "train Loss: 1.217 | Acc: 56.422% (27805/49280)\n",
      "test Loss: 1.040 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 26\n",
      "train Loss: 1.187 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.213 | Acc: 56.504% (9330/16512)\n",
      "train Loss: 1.205 | Acc: 57.001% (18751/32896)\n",
      "train Loss: 1.209 | Acc: 56.755% (27969/49280)\n",
      "test Loss: 1.039 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 27\n",
      "train Loss: 1.144 | Acc: 57.031% (73/128)\n",
      "train Loss: 1.194 | Acc: 57.298% (9461/16512)\n",
      "train Loss: 1.195 | Acc: 57.460% (18902/32896)\n",
      "train Loss: 1.195 | Acc: 57.413% (28293/49280)\n",
      "test Loss: 1.057 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 28\n",
      "train Loss: 1.262 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.192 | Acc: 57.292% (9460/16512)\n",
      "train Loss: 1.187 | Acc: 57.600% (18948/32896)\n",
      "train Loss: 1.189 | Acc: 57.449% (28311/49280)\n",
      "test Loss: 1.041 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 29\n",
      "train Loss: 1.306 | Acc: 56.250% (72/128)\n",
      "train Loss: 1.172 | Acc: 57.994% (9576/16512)\n",
      "train Loss: 1.170 | Acc: 58.071% (19103/32896)\n",
      "train Loss: 1.175 | Acc: 57.948% (28557/49280)\n",
      "test Loss: 1.057 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 30\n",
      "train Loss: 1.012 | Acc: 67.188% (86/128)\n",
      "train Loss: 1.157 | Acc: 58.709% (9694/16512)\n",
      "train Loss: 1.151 | Acc: 58.861% (19363/32896)\n",
      "train Loss: 1.163 | Acc: 58.584% (28870/49280)\n",
      "test Loss: 0.992 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 31\n",
      "train Loss: 1.188 | Acc: 53.125% (68/128)\n",
      "train Loss: 1.164 | Acc: 58.370% (9638/16512)\n",
      "train Loss: 1.160 | Acc: 58.639% (19290/32896)\n",
      "train Loss: 1.158 | Acc: 58.728% (28941/49280)\n",
      "test Loss: 0.975 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 32\n",
      "train Loss: 1.118 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.141 | Acc: 59.296% (9791/16512)\n",
      "train Loss: 1.149 | Acc: 59.247% (19490/32896)\n",
      "train Loss: 1.146 | Acc: 59.347% (29246/49280)\n",
      "test Loss: 0.967 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 33\n",
      "train Loss: 1.102 | Acc: 60.156% (77/128)\n",
      "train Loss: 1.137 | Acc: 59.787% (9872/16512)\n",
      "train Loss: 1.141 | Acc: 59.524% (19581/32896)\n",
      "train Loss: 1.137 | Acc: 59.659% (29400/49280)\n",
      "test Loss: 0.997 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 34\n",
      "train Loss: 1.164 | Acc: 60.156% (77/128)\n",
      "train Loss: 1.139 | Acc: 59.872% (9886/16512)\n",
      "train Loss: 1.134 | Acc: 59.898% (19704/32896)\n",
      "train Loss: 1.135 | Acc: 59.702% (29421/49280)\n",
      "test Loss: 0.929 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 35\n",
      "train Loss: 1.089 | Acc: 60.938% (78/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.140 | Acc: 59.466% (9819/16512)\n",
      "train Loss: 1.130 | Acc: 59.560% (19593/32896)\n",
      "train Loss: 1.122 | Acc: 59.923% (29530/49280)\n",
      "test Loss: 0.978 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 36\n",
      "train Loss: 1.023 | Acc: 63.281% (81/128)\n",
      "train Loss: 1.107 | Acc: 60.556% (9999/16512)\n",
      "train Loss: 1.115 | Acc: 60.168% (19793/32896)\n",
      "train Loss: 1.116 | Acc: 60.213% (29673/49280)\n",
      "test Loss: 0.936 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 37\n",
      "train Loss: 1.123 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.106 | Acc: 60.816% (10042/16512)\n",
      "train Loss: 1.113 | Acc: 60.506% (19904/32896)\n",
      "train Loss: 1.109 | Acc: 60.623% (29875/49280)\n",
      "test Loss: 0.980 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 38\n",
      "train Loss: 1.027 | Acc: 60.938% (78/128)\n",
      "train Loss: 1.101 | Acc: 60.538% (9996/16512)\n",
      "train Loss: 1.103 | Acc: 60.597% (19934/32896)\n",
      "train Loss: 1.099 | Acc: 60.840% (29982/49280)\n",
      "test Loss: 0.973 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 39\n",
      "train Loss: 1.149 | Acc: 58.594% (75/128)\n",
      "train Loss: 1.090 | Acc: 61.252% (10114/16512)\n",
      "train Loss: 1.093 | Acc: 61.141% (20113/32896)\n",
      "train Loss: 1.091 | Acc: 61.216% (30167/49280)\n",
      "test Loss: 0.920 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 40\n",
      "train Loss: 1.164 | Acc: 56.250% (72/128)\n",
      "train Loss: 1.071 | Acc: 62.009% (10239/16512)\n",
      "train Loss: 1.082 | Acc: 61.506% (20233/32896)\n",
      "train Loss: 1.083 | Acc: 61.498% (30306/49280)\n",
      "test Loss: 0.905 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 41\n",
      "train Loss: 1.134 | Acc: 63.281% (81/128)\n",
      "train Loss: 1.080 | Acc: 61.598% (10171/16512)\n",
      "train Loss: 1.081 | Acc: 61.783% (20324/32896)\n",
      "train Loss: 1.077 | Acc: 61.753% (30432/49280)\n",
      "test Loss: 0.912 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 42\n",
      "train Loss: 1.005 | Acc: 60.156% (77/128)\n",
      "train Loss: 1.073 | Acc: 61.755% (10197/16512)\n",
      "train Loss: 1.065 | Acc: 62.029% (20405/32896)\n",
      "train Loss: 1.068 | Acc: 61.800% (30455/49280)\n",
      "test Loss: 0.923 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 43\n",
      "train Loss: 1.133 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.056 | Acc: 62.458% (10313/16512)\n",
      "train Loss: 1.061 | Acc: 62.129% (20438/32896)\n",
      "train Loss: 1.063 | Acc: 62.145% (30625/49280)\n",
      "test Loss: 0.907 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 44\n",
      "train Loss: 1.081 | Acc: 56.250% (72/128)\n",
      "train Loss: 1.048 | Acc: 62.518% (10323/16512)\n",
      "train Loss: 1.052 | Acc: 62.336% (20506/32896)\n",
      "train Loss: 1.049 | Acc: 62.518% (30809/49280)\n",
      "test Loss: 0.894 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 45\n",
      "train Loss: 1.238 | Acc: 59.375% (76/128)\n",
      "train Loss: 1.031 | Acc: 63.396% (10468/16512)\n",
      "train Loss: 1.040 | Acc: 63.093% (20755/32896)\n",
      "train Loss: 1.044 | Acc: 62.918% (31006/49280)\n",
      "test Loss: 0.888 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 46\n",
      "train Loss: 1.043 | Acc: 60.938% (78/128)\n",
      "train Loss: 1.040 | Acc: 63.172% (10431/16512)\n",
      "train Loss: 1.041 | Acc: 63.132% (20768/32896)\n",
      "train Loss: 1.040 | Acc: 63.196% (31143/49280)\n",
      "test Loss: 0.920 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 47\n",
      "train Loss: 1.018 | Acc: 63.281% (81/128)\n",
      "train Loss: 1.040 | Acc: 63.057% (10412/16512)\n",
      "train Loss: 1.040 | Acc: 63.120% (20764/32896)\n",
      "train Loss: 1.038 | Acc: 63.377% (31232/49280)\n",
      "test Loss: 0.929 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 48\n",
      "train Loss: 0.973 | Acc: 67.188% (86/128)\n",
      "train Loss: 1.029 | Acc: 63.578% (10498/16512)\n",
      "train Loss: 1.030 | Acc: 63.534% (20900/32896)\n",
      "train Loss: 1.029 | Acc: 63.563% (31324/49280)\n",
      "test Loss: 0.884 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 49\n",
      "train Loss: 1.094 | Acc: 60.938% (78/128)\n",
      "train Loss: 1.020 | Acc: 63.941% (10558/16512)\n",
      "train Loss: 1.019 | Acc: 63.783% (20982/32896)\n",
      "train Loss: 1.022 | Acc: 63.750% (31416/49280)\n",
      "test Loss: 0.848 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 50\n",
      "train Loss: 0.974 | Acc: 67.969% (87/128)\n",
      "train Loss: 1.018 | Acc: 63.542% (10492/16512)\n",
      "train Loss: 1.010 | Acc: 64.059% (21073/32896)\n",
      "train Loss: 1.013 | Acc: 63.957% (31518/49280)\n",
      "test Loss: 0.857 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 51\n",
      "train Loss: 0.942 | Acc: 61.719% (79/128)\n",
      "train Loss: 1.008 | Acc: 64.123% (10588/16512)\n",
      "train Loss: 1.007 | Acc: 64.151% (21103/32896)\n",
      "train Loss: 1.008 | Acc: 64.154% (31615/49280)\n",
      "test Loss: 0.840 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 52\n",
      "train Loss: 0.899 | Acc: 71.094% (91/128)\n",
      "train Loss: 1.008 | Acc: 64.123% (10588/16512)\n",
      "train Loss: 1.007 | Acc: 64.214% (21124/32896)\n",
      "train Loss: 1.005 | Acc: 64.324% (31699/49280)\n",
      "test Loss: 0.855 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 53\n",
      "train Loss: 1.030 | Acc: 65.625% (84/128)\n",
      "train Loss: 0.995 | Acc: 64.989% (10731/16512)\n",
      "train Loss: 0.995 | Acc: 64.810% (21320/32896)\n",
      "train Loss: 0.996 | Acc: 64.775% (31921/49280)\n",
      "test Loss: 0.814 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 54\n",
      "train Loss: 1.014 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.982 | Acc: 65.062% (10743/16512)\n",
      "train Loss: 0.990 | Acc: 64.877% (21342/32896)\n",
      "train Loss: 0.992 | Acc: 64.793% (31930/49280)\n",
      "test Loss: 0.824 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 55\n",
      "train Loss: 0.997 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.978 | Acc: 65.443% (10806/16512)\n",
      "train Loss: 0.986 | Acc: 64.965% (21371/32896)\n",
      "train Loss: 0.990 | Acc: 64.838% (31952/49280)\n",
      "test Loss: 0.864 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 56\n",
      "train Loss: 0.811 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.983 | Acc: 64.856% (10709/16512)\n",
      "train Loss: 0.977 | Acc: 65.278% (21474/32896)\n",
      "train Loss: 0.982 | Acc: 65.075% (32069/49280)\n",
      "test Loss: 0.857 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 57\n",
      "train Loss: 1.089 | Acc: 60.938% (78/128)\n",
      "train Loss: 0.971 | Acc: 65.231% (10771/16512)\n",
      "train Loss: 0.974 | Acc: 65.339% (21494/32896)\n",
      "train Loss: 0.979 | Acc: 65.132% (32097/49280)\n",
      "test Loss: 0.845 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 58\n",
      "train Loss: 1.029 | Acc: 62.500% (80/128)\n",
      "train Loss: 0.970 | Acc: 65.365% (10793/16512)\n",
      "train Loss: 0.968 | Acc: 65.750% (21629/32896)\n",
      "train Loss: 0.966 | Acc: 65.696% (32375/49280)\n",
      "test Loss: 0.847 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 59\n",
      "train Loss: 0.852 | Acc: 67.969% (87/128)\n",
      "train Loss: 0.960 | Acc: 66.109% (10916/16512)\n",
      "train Loss: 0.962 | Acc: 66.157% (21763/32896)\n",
      "train Loss: 0.965 | Acc: 65.938% (32494/49280)\n",
      "test Loss: 0.846 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 60\n",
      "train Loss: 1.054 | Acc: 64.844% (83/128)\n",
      "train Loss: 0.953 | Acc: 66.279% (10944/16512)\n",
      "train Loss: 0.955 | Acc: 66.276% (21802/32896)\n",
      "train Loss: 0.960 | Acc: 66.096% (32572/49280)\n",
      "test Loss: 0.836 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 61\n",
      "train Loss: 0.795 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.957 | Acc: 65.752% (10857/16512)\n",
      "train Loss: 0.952 | Acc: 66.181% (21771/32896)\n",
      "train Loss: 0.952 | Acc: 66.144% (32596/49280)\n",
      "test Loss: 0.816 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 62\n",
      "train Loss: 0.874 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.950 | Acc: 66.194% (10930/16512)\n",
      "train Loss: 0.946 | Acc: 66.592% (21906/32896)\n",
      "train Loss: 0.943 | Acc: 66.619% (32830/49280)\n",
      "test Loss: 0.862 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 63\n",
      "train Loss: 0.912 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.937 | Acc: 66.927% (11051/16512)\n",
      "train Loss: 0.945 | Acc: 66.412% (21847/32896)\n",
      "train Loss: 0.946 | Acc: 66.406% (32725/49280)\n",
      "test Loss: 0.816 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 64\n",
      "train Loss: 0.936 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.941 | Acc: 66.818% (11033/16512)\n",
      "train Loss: 0.938 | Acc: 66.963% (22028/32896)\n",
      "train Loss: 0.938 | Acc: 66.934% (32985/49280)\n",
      "test Loss: 0.852 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 65\n",
      "train Loss: 0.950 | Acc: 64.844% (83/128)\n",
      "train Loss: 0.948 | Acc: 66.007% (10899/16512)\n",
      "train Loss: 0.940 | Acc: 66.586% (21904/32896)\n",
      "train Loss: 0.937 | Acc: 66.674% (32857/49280)\n",
      "test Loss: 0.841 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 66\n",
      "train Loss: 1.063 | Acc: 62.500% (80/128)\n",
      "train Loss: 0.927 | Acc: 66.969% (11058/16512)\n",
      "train Loss: 0.925 | Acc: 67.285% (22134/32896)\n",
      "train Loss: 0.930 | Acc: 67.121% (33077/49280)\n",
      "test Loss: 0.835 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 67\n",
      "train Loss: 0.940 | Acc: 65.625% (84/128)\n",
      "train Loss: 0.926 | Acc: 67.224% (11100/16512)\n",
      "train Loss: 0.929 | Acc: 67.133% (22084/32896)\n",
      "train Loss: 0.925 | Acc: 67.346% (33188/49280)\n",
      "test Loss: 0.817 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 68\n",
      "train Loss: 0.887 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.919 | Acc: 67.624% (11166/16512)\n",
      "train Loss: 0.916 | Acc: 67.792% (22301/32896)\n",
      "train Loss: 0.925 | Acc: 67.510% (33269/49280)\n",
      "test Loss: 0.838 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 69\n",
      "train Loss: 0.863 | Acc: 67.969% (87/128)\n",
      "train Loss: 0.921 | Acc: 67.442% (11136/16512)\n",
      "train Loss: 0.923 | Acc: 67.349% (22155/32896)\n",
      "train Loss: 0.915 | Acc: 67.551% (33289/49280)\n",
      "test Loss: 0.848 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 70\n",
      "train Loss: 0.828 | Acc: 67.969% (87/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.915 | Acc: 67.617% (11165/16512)\n",
      "train Loss: 0.909 | Acc: 67.799% (22303/32896)\n",
      "train Loss: 0.915 | Acc: 67.581% (33304/49280)\n",
      "test Loss: 0.844 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 71\n",
      "train Loss: 0.798 | Acc: 77.344% (99/128)\n",
      "train Loss: 0.909 | Acc: 67.745% (11186/16512)\n",
      "train Loss: 0.913 | Acc: 67.707% (22273/32896)\n",
      "train Loss: 0.910 | Acc: 67.877% (33450/49280)\n",
      "test Loss: 0.813 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 72\n",
      "train Loss: 1.005 | Acc: 59.375% (76/128)\n",
      "train Loss: 0.903 | Acc: 68.496% (11310/16512)\n",
      "train Loss: 0.908 | Acc: 67.993% (22367/32896)\n",
      "train Loss: 0.908 | Acc: 67.999% (33510/49280)\n",
      "test Loss: 0.813 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 73\n",
      "train Loss: 0.829 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.899 | Acc: 68.411% (11296/16512)\n",
      "train Loss: 0.900 | Acc: 68.577% (22559/32896)\n",
      "train Loss: 0.902 | Acc: 68.488% (33751/49280)\n",
      "test Loss: 0.851 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 74\n",
      "train Loss: 0.863 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.903 | Acc: 67.981% (11225/16512)\n",
      "train Loss: 0.898 | Acc: 68.109% (22405/32896)\n",
      "train Loss: 0.899 | Acc: 68.184% (33601/49280)\n",
      "test Loss: 0.855 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 75\n",
      "train Loss: 0.884 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.893 | Acc: 68.671% (11339/16512)\n",
      "train Loss: 0.889 | Acc: 68.704% (22601/32896)\n",
      "train Loss: 0.893 | Acc: 68.476% (33745/49280)\n",
      "test Loss: 0.850 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 76\n",
      "train Loss: 0.668 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.896 | Acc: 68.381% (11291/16512)\n",
      "train Loss: 0.898 | Acc: 68.309% (22471/32896)\n",
      "train Loss: 0.894 | Acc: 68.334% (33675/49280)\n",
      "test Loss: 0.854 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 77\n",
      "train Loss: 0.880 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.871 | Acc: 68.938% (11383/16512)\n",
      "train Loss: 0.880 | Acc: 68.747% (22615/32896)\n",
      "train Loss: 0.889 | Acc: 68.468% (33741/49280)\n",
      "test Loss: 0.862 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 78\n",
      "train Loss: 0.817 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.887 | Acc: 68.732% (11349/16512)\n",
      "train Loss: 0.889 | Acc: 68.735% (22611/32896)\n",
      "train Loss: 0.887 | Acc: 68.872% (33940/49280)\n",
      "test Loss: 0.825 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 79\n",
      "train Loss: 0.796 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.871 | Acc: 69.053% (11402/16512)\n",
      "train Loss: 0.876 | Acc: 68.905% (22667/32896)\n",
      "train Loss: 0.883 | Acc: 68.874% (33941/49280)\n",
      "test Loss: 0.860 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 80\n",
      "train Loss: 0.697 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.876 | Acc: 68.689% (11342/16512)\n",
      "train Loss: 0.879 | Acc: 68.710% (22603/32896)\n",
      "train Loss: 0.882 | Acc: 68.699% (33855/49280)\n",
      "test Loss: 0.883 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 81\n",
      "train Loss: 0.914 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.886 | Acc: 68.599% (11327/16512)\n",
      "train Loss: 0.876 | Acc: 69.036% (22710/32896)\n",
      "train Loss: 0.878 | Acc: 68.862% (33935/49280)\n",
      "test Loss: 0.824 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 82\n",
      "train Loss: 0.874 | Acc: 67.969% (87/128)\n",
      "train Loss: 0.876 | Acc: 69.537% (11482/16512)\n",
      "train Loss: 0.876 | Acc: 69.224% (22772/32896)\n",
      "train Loss: 0.875 | Acc: 69.209% (34106/49280)\n",
      "test Loss: 0.841 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 83\n",
      "train Loss: 0.950 | Acc: 67.969% (87/128)\n",
      "train Loss: 0.871 | Acc: 69.216% (11429/16512)\n",
      "train Loss: 0.872 | Acc: 69.318% (22803/32896)\n",
      "train Loss: 0.873 | Acc: 69.251% (34127/49280)\n",
      "test Loss: 0.840 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 84\n",
      "train Loss: 0.769 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.874 | Acc: 69.198% (11426/16512)\n",
      "train Loss: 0.872 | Acc: 69.276% (22789/32896)\n",
      "train Loss: 0.869 | Acc: 69.310% (34156/49280)\n",
      "test Loss: 0.852 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 85\n",
      "train Loss: 0.880 | Acc: 67.969% (87/128)\n",
      "train Loss: 0.879 | Acc: 68.368% (11289/16512)\n",
      "train Loss: 0.879 | Acc: 68.665% (22588/32896)\n",
      "train Loss: 0.869 | Acc: 69.107% (34056/49280)\n",
      "test Loss: 0.823 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 86\n",
      "train Loss: 0.938 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.864 | Acc: 69.083% (11407/16512)\n",
      "train Loss: 0.871 | Acc: 69.036% (22710/32896)\n",
      "train Loss: 0.870 | Acc: 69.138% (34071/49280)\n",
      "test Loss: 0.812 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 87\n",
      "train Loss: 0.756 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.858 | Acc: 69.773% (11521/16512)\n",
      "train Loss: 0.861 | Acc: 69.635% (22907/32896)\n",
      "train Loss: 0.857 | Acc: 69.704% (34350/49280)\n",
      "test Loss: 0.886 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 88\n",
      "train Loss: 0.856 | Acc: 64.062% (82/128)\n",
      "train Loss: 0.847 | Acc: 70.306% (11609/16512)\n",
      "train Loss: 0.855 | Acc: 69.747% (22944/32896)\n",
      "train Loss: 0.860 | Acc: 69.716% (34356/49280)\n",
      "test Loss: 0.826 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 89\n",
      "train Loss: 0.846 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.861 | Acc: 69.489% (11474/16512)\n",
      "train Loss: 0.861 | Acc: 69.525% (22871/32896)\n",
      "train Loss: 0.863 | Acc: 69.448% (34224/49280)\n",
      "test Loss: 0.839 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 90\n",
      "train Loss: 0.861 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.838 | Acc: 70.331% (11613/16512)\n",
      "train Loss: 0.852 | Acc: 69.777% (22954/32896)\n",
      "train Loss: 0.851 | Acc: 70.022% (34507/49280)\n",
      "test Loss: 0.825 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 91\n",
      "train Loss: 0.798 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.851 | Acc: 70.010% (11560/16512)\n",
      "train Loss: 0.861 | Acc: 69.625% (22904/32896)\n",
      "train Loss: 0.855 | Acc: 69.966% (34479/49280)\n",
      "test Loss: 0.806 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 92\n",
      "train Loss: 0.870 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.851 | Acc: 69.961% (11552/16512)\n",
      "train Loss: 0.848 | Acc: 70.063% (23048/32896)\n",
      "train Loss: 0.851 | Acc: 70.085% (34538/49280)\n",
      "test Loss: 0.847 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 93\n",
      "train Loss: 0.948 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.841 | Acc: 70.633% (11663/16512)\n",
      "train Loss: 0.842 | Acc: 70.458% (23178/32896)\n",
      "train Loss: 0.846 | Acc: 70.321% (34654/49280)\n",
      "test Loss: 0.843 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 94\n",
      "train Loss: 0.754 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.851 | Acc: 70.022% (11562/16512)\n",
      "train Loss: 0.839 | Acc: 70.410% (23162/32896)\n",
      "train Loss: 0.847 | Acc: 70.199% (34594/49280)\n",
      "test Loss: 0.883 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 95\n",
      "train Loss: 0.834 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.838 | Acc: 70.397% (11624/16512)\n",
      "train Loss: 0.838 | Acc: 70.312% (23130/32896)\n",
      "train Loss: 0.840 | Acc: 70.254% (34621/49280)\n",
      "test Loss: 0.900 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 96\n",
      "train Loss: 1.004 | Acc: 64.062% (82/128)\n",
      "train Loss: 0.846 | Acc: 69.852% (11534/16512)\n",
      "train Loss: 0.839 | Acc: 70.340% (23139/32896)\n",
      "train Loss: 0.840 | Acc: 70.294% (34641/49280)\n",
      "test Loss: 0.835 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 97\n",
      "train Loss: 0.803 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.830 | Acc: 70.845% (11698/16512)\n",
      "train Loss: 0.836 | Acc: 70.638% (23237/32896)\n",
      "train Loss: 0.840 | Acc: 70.524% (34754/49280)\n",
      "test Loss: 0.822 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 98\n",
      "train Loss: 0.886 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.839 | Acc: 70.773% (11686/16512)\n",
      "train Loss: 0.837 | Acc: 70.592% (23222/32896)\n",
      "train Loss: 0.835 | Acc: 70.659% (34821/49280)\n",
      "test Loss: 0.873 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 99\n",
      "train Loss: 0.868 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.843 | Acc: 69.943% (11549/16512)\n",
      "train Loss: 0.837 | Acc: 70.431% (23169/32896)\n",
      "train Loss: 0.836 | Acc: 70.469% (34727/49280)\n",
      "test Loss: 0.850 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 100\n",
      "train Loss: 1.042 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.843 | Acc: 70.676% (11670/16512)\n",
      "train Loss: 0.838 | Acc: 70.589% (23221/32896)\n",
      "train Loss: 0.835 | Acc: 70.716% (34849/49280)\n",
      "test Loss: 0.816 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 101\n",
      "train Loss: 0.697 | Acc: 76.562% (98/128)\n",
      "train Loss: 0.825 | Acc: 71.088% (11738/16512)\n",
      "train Loss: 0.824 | Acc: 71.000% (23356/32896)\n",
      "train Loss: 0.831 | Acc: 70.820% (34900/49280)\n",
      "test Loss: 0.864 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 102\n",
      "train Loss: 0.868 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.823 | Acc: 71.106% (11741/16512)\n",
      "train Loss: 0.826 | Acc: 70.838% (23303/32896)\n",
      "train Loss: 0.827 | Acc: 70.735% (34858/49280)\n",
      "test Loss: 0.844 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 103\n",
      "train Loss: 0.792 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.830 | Acc: 70.864% (11701/16512)\n",
      "train Loss: 0.834 | Acc: 70.778% (23283/32896)\n",
      "train Loss: 0.828 | Acc: 70.935% (34957/49280)\n",
      "test Loss: 0.864 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 104\n",
      "train Loss: 0.867 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.820 | Acc: 71.506% (11807/16512)\n",
      "train Loss: 0.822 | Acc: 71.118% (23395/32896)\n",
      "train Loss: 0.824 | Acc: 71.059% (35018/49280)\n",
      "test Loss: 0.841 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 105\n",
      "train Loss: 0.830 | Acc: 72.656% (93/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.819 | Acc: 71.076% (11736/16512)\n",
      "train Loss: 0.827 | Acc: 70.945% (23338/32896)\n",
      "train Loss: 0.825 | Acc: 70.938% (34958/49280)\n",
      "test Loss: 0.926 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 106\n",
      "train Loss: 0.672 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.829 | Acc: 70.985% (11721/16512)\n",
      "train Loss: 0.823 | Acc: 71.255% (23440/32896)\n",
      "train Loss: 0.822 | Acc: 71.132% (35054/49280)\n",
      "test Loss: 0.842 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 107\n",
      "train Loss: 0.828 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.805 | Acc: 71.730% (11844/16512)\n",
      "train Loss: 0.812 | Acc: 71.483% (23515/32896)\n",
      "train Loss: 0.818 | Acc: 71.264% (35119/49280)\n",
      "test Loss: 0.848 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 108\n",
      "train Loss: 0.908 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.809 | Acc: 71.500% (11806/16512)\n",
      "train Loss: 0.814 | Acc: 71.337% (23467/32896)\n",
      "train Loss: 0.815 | Acc: 71.313% (35143/49280)\n",
      "test Loss: 0.858 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 109\n",
      "train Loss: 0.906 | Acc: 66.406% (85/128)\n",
      "train Loss: 0.810 | Acc: 71.512% (11808/16512)\n",
      "train Loss: 0.816 | Acc: 71.449% (23504/32896)\n",
      "train Loss: 0.819 | Acc: 71.329% (35151/49280)\n",
      "test Loss: 0.834 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 110\n",
      "train Loss: 0.801 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.810 | Acc: 71.100% (11740/16512)\n",
      "train Loss: 0.818 | Acc: 70.872% (23314/32896)\n",
      "train Loss: 0.817 | Acc: 70.970% (34974/49280)\n",
      "test Loss: 0.821 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 111\n",
      "train Loss: 0.823 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.797 | Acc: 72.087% (11903/16512)\n",
      "train Loss: 0.805 | Acc: 71.711% (23590/32896)\n",
      "train Loss: 0.809 | Acc: 71.688% (35328/49280)\n",
      "test Loss: 0.883 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 112\n",
      "train Loss: 0.765 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.806 | Acc: 71.409% (11791/16512)\n",
      "train Loss: 0.810 | Acc: 71.346% (23470/32896)\n",
      "train Loss: 0.810 | Acc: 71.496% (35233/49280)\n",
      "test Loss: 0.853 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 113\n",
      "train Loss: 0.653 | Acc: 80.469% (103/128)\n",
      "train Loss: 0.805 | Acc: 71.911% (11874/16512)\n",
      "train Loss: 0.809 | Acc: 71.647% (23569/32896)\n",
      "train Loss: 0.810 | Acc: 71.581% (35275/49280)\n",
      "test Loss: 0.863 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 114\n",
      "train Loss: 0.760 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.802 | Acc: 71.784% (11853/16512)\n",
      "train Loss: 0.800 | Acc: 71.930% (23662/32896)\n",
      "train Loss: 0.806 | Acc: 71.640% (35304/49280)\n",
      "test Loss: 0.878 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 115\n",
      "train Loss: 0.765 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.793 | Acc: 72.341% (11945/16512)\n",
      "train Loss: 0.803 | Acc: 71.872% (23643/32896)\n",
      "train Loss: 0.804 | Acc: 71.893% (35429/49280)\n",
      "test Loss: 0.847 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 116\n",
      "train Loss: 0.735 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.805 | Acc: 71.972% (11884/16512)\n",
      "train Loss: 0.804 | Acc: 71.860% (23639/32896)\n",
      "train Loss: 0.804 | Acc: 71.867% (35416/49280)\n",
      "test Loss: 0.819 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 117\n",
      "train Loss: 0.682 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.803 | Acc: 71.724% (11843/16512)\n",
      "train Loss: 0.804 | Acc: 71.759% (23606/32896)\n",
      "train Loss: 0.803 | Acc: 71.838% (35402/49280)\n",
      "test Loss: 0.823 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 118\n",
      "train Loss: 0.936 | Acc: 65.625% (84/128)\n",
      "train Loss: 0.805 | Acc: 71.857% (11865/16512)\n",
      "train Loss: 0.802 | Acc: 71.997% (23684/32896)\n",
      "train Loss: 0.800 | Acc: 72.052% (35507/49280)\n",
      "test Loss: 0.855 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 119\n",
      "train Loss: 0.819 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.796 | Acc: 72.329% (11943/16512)\n",
      "train Loss: 0.806 | Acc: 71.860% (23639/32896)\n",
      "train Loss: 0.805 | Acc: 71.784% (35375/49280)\n",
      "test Loss: 0.809 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 120\n",
      "train Loss: 0.768 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.804 | Acc: 71.590% (11821/16512)\n",
      "train Loss: 0.796 | Acc: 71.969% (23675/32896)\n",
      "train Loss: 0.798 | Acc: 71.897% (35431/49280)\n",
      "test Loss: 0.780 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 121\n",
      "train Loss: 0.796 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.791 | Acc: 72.202% (11922/16512)\n",
      "train Loss: 0.794 | Acc: 72.161% (23738/32896)\n",
      "train Loss: 0.796 | Acc: 72.084% (35523/49280)\n",
      "test Loss: 0.849 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 122\n",
      "train Loss: 0.740 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.787 | Acc: 72.341% (11945/16512)\n",
      "train Loss: 0.794 | Acc: 72.130% (23728/32896)\n",
      "train Loss: 0.795 | Acc: 72.023% (35493/49280)\n",
      "test Loss: 0.832 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 123\n",
      "train Loss: 0.829 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.792 | Acc: 72.390% (11953/16512)\n",
      "train Loss: 0.796 | Acc: 72.276% (23776/32896)\n",
      "train Loss: 0.798 | Acc: 71.985% (35474/49280)\n",
      "test Loss: 0.855 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 124\n",
      "train Loss: 0.871 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.790 | Acc: 71.930% (11877/16512)\n",
      "train Loss: 0.792 | Acc: 71.978% (23678/32896)\n",
      "train Loss: 0.796 | Acc: 71.855% (35410/49280)\n",
      "test Loss: 0.856 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 125\n",
      "train Loss: 0.950 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.783 | Acc: 72.269% (11933/16512)\n",
      "train Loss: 0.785 | Acc: 72.206% (23753/32896)\n",
      "train Loss: 0.786 | Acc: 72.358% (35658/49280)\n",
      "test Loss: 0.852 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 126\n",
      "train Loss: 0.733 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.812 | Acc: 71.348% (11781/16512)\n",
      "train Loss: 0.799 | Acc: 71.769% (23609/32896)\n",
      "train Loss: 0.795 | Acc: 71.934% (35449/49280)\n",
      "test Loss: 0.819 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 127\n",
      "train Loss: 0.702 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.776 | Acc: 72.953% (12046/16512)\n",
      "train Loss: 0.777 | Acc: 72.784% (23943/32896)\n",
      "train Loss: 0.782 | Acc: 72.650% (35802/49280)\n",
      "test Loss: 0.865 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 128\n",
      "train Loss: 0.931 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.786 | Acc: 72.317% (11941/16512)\n",
      "train Loss: 0.785 | Acc: 72.522% (23857/32896)\n",
      "train Loss: 0.789 | Acc: 72.394% (35676/49280)\n",
      "test Loss: 0.814 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 129\n",
      "train Loss: 0.726 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.772 | Acc: 72.802% (12021/16512)\n",
      "train Loss: 0.778 | Acc: 72.565% (23871/32896)\n",
      "train Loss: 0.782 | Acc: 72.397% (35677/49280)\n",
      "test Loss: 0.893 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 130\n",
      "train Loss: 0.854 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.792 | Acc: 72.129% (11910/16512)\n",
      "train Loss: 0.783 | Acc: 72.535% (23861/32896)\n",
      "train Loss: 0.782 | Acc: 72.595% (35775/49280)\n",
      "test Loss: 0.849 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 131\n",
      "train Loss: 0.824 | Acc: 64.844% (83/128)\n",
      "train Loss: 0.787 | Acc: 71.972% (11884/16512)\n",
      "train Loss: 0.785 | Acc: 72.173% (23742/32896)\n",
      "train Loss: 0.778 | Acc: 72.561% (35758/49280)\n",
      "test Loss: 0.874 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 132\n",
      "train Loss: 0.721 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.760 | Acc: 73.353% (12112/16512)\n",
      "train Loss: 0.775 | Acc: 72.945% (23996/32896)\n",
      "train Loss: 0.782 | Acc: 72.672% (35813/49280)\n",
      "test Loss: 0.828 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 133\n",
      "train Loss: 0.823 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.780 | Acc: 72.662% (11998/16512)\n",
      "train Loss: 0.777 | Acc: 72.875% (23973/32896)\n",
      "train Loss: 0.779 | Acc: 72.819% (35885/49280)\n",
      "test Loss: 0.866 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 134\n",
      "train Loss: 0.747 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.782 | Acc: 72.359% (11948/16512)\n",
      "train Loss: 0.783 | Acc: 72.635% (23894/32896)\n",
      "train Loss: 0.781 | Acc: 72.741% (35847/49280)\n",
      "test Loss: 0.841 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 135\n",
      "train Loss: 0.712 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.774 | Acc: 72.777% (12017/16512)\n",
      "train Loss: 0.779 | Acc: 72.547% (23865/32896)\n",
      "train Loss: 0.781 | Acc: 72.494% (35725/49280)\n",
      "test Loss: 0.859 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 136\n",
      "train Loss: 0.788 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.771 | Acc: 72.668% (11999/16512)\n",
      "train Loss: 0.775 | Acc: 72.647% (23898/32896)\n",
      "train Loss: 0.780 | Acc: 72.463% (35710/49280)\n",
      "test Loss: 0.854 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 137\n",
      "train Loss: 0.814 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.777 | Acc: 72.717% (12007/16512)\n",
      "train Loss: 0.770 | Acc: 72.985% (24009/32896)\n",
      "train Loss: 0.772 | Acc: 72.999% (35974/49280)\n",
      "test Loss: 0.854 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 138\n",
      "train Loss: 0.818 | Acc: 64.844% (83/128)\n",
      "train Loss: 0.764 | Acc: 73.129% (12075/16512)\n",
      "train Loss: 0.764 | Acc: 72.893% (23979/32896)\n",
      "train Loss: 0.772 | Acc: 72.715% (35834/49280)\n",
      "test Loss: 0.812 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 139\n",
      "train Loss: 0.800 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.758 | Acc: 72.892% (12036/16512)\n",
      "train Loss: 0.768 | Acc: 72.775% (23940/32896)\n",
      "train Loss: 0.775 | Acc: 72.632% (35793/49280)\n",
      "test Loss: 0.856 | Acc: 68.000% (68/100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 140\n",
      "train Loss: 0.972 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.772 | Acc: 73.141% (12077/16512)\n",
      "train Loss: 0.778 | Acc: 72.781% (23942/32896)\n",
      "train Loss: 0.775 | Acc: 72.800% (35876/49280)\n",
      "test Loss: 0.816 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 141\n",
      "train Loss: 0.956 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.759 | Acc: 73.468% (12131/16512)\n",
      "train Loss: 0.764 | Acc: 73.085% (24042/32896)\n",
      "train Loss: 0.767 | Acc: 73.011% (35980/49280)\n",
      "test Loss: 0.820 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 142\n",
      "train Loss: 0.799 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.775 | Acc: 72.481% (11968/16512)\n",
      "train Loss: 0.774 | Acc: 72.723% (23923/32896)\n",
      "train Loss: 0.772 | Acc: 72.794% (35873/49280)\n",
      "test Loss: 0.798 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 143\n",
      "train Loss: 0.644 | Acc: 78.906% (101/128)\n",
      "train Loss: 0.757 | Acc: 73.286% (12101/16512)\n",
      "train Loss: 0.774 | Acc: 72.881% (23975/32896)\n",
      "train Loss: 0.767 | Acc: 73.072% (36010/49280)\n",
      "test Loss: 0.780 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 144\n",
      "train Loss: 0.714 | Acc: 76.562% (98/128)\n",
      "train Loss: 0.764 | Acc: 72.947% (12045/16512)\n",
      "train Loss: 0.765 | Acc: 72.982% (24008/32896)\n",
      "train Loss: 0.767 | Acc: 73.022% (35985/49280)\n",
      "test Loss: 0.844 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 145\n",
      "train Loss: 0.800 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.764 | Acc: 73.286% (12101/16512)\n",
      "train Loss: 0.763 | Acc: 73.374% (24137/32896)\n",
      "train Loss: 0.764 | Acc: 73.291% (36118/49280)\n",
      "test Loss: 0.828 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 146\n",
      "train Loss: 0.693 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.756 | Acc: 73.195% (12086/16512)\n",
      "train Loss: 0.763 | Acc: 73.182% (24074/32896)\n",
      "train Loss: 0.764 | Acc: 73.186% (36066/49280)\n",
      "test Loss: 0.780 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 147\n",
      "train Loss: 0.816 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.763 | Acc: 73.492% (12135/16512)\n",
      "train Loss: 0.760 | Acc: 73.596% (24210/32896)\n",
      "train Loss: 0.762 | Acc: 73.488% (36215/49280)\n",
      "test Loss: 0.799 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 148\n",
      "train Loss: 0.740 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.749 | Acc: 73.934% (12208/16512)\n",
      "train Loss: 0.758 | Acc: 73.434% (24157/32896)\n",
      "train Loss: 0.763 | Acc: 73.202% (36074/49280)\n",
      "test Loss: 0.842 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 149\n",
      "train Loss: 0.753 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.753 | Acc: 73.771% (12181/16512)\n",
      "train Loss: 0.751 | Acc: 73.675% (24236/32896)\n",
      "train Loss: 0.761 | Acc: 73.279% (36112/49280)\n",
      "test Loss: 0.757 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 150\n",
      "train Loss: 0.663 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.757 | Acc: 73.189% (12085/16512)\n",
      "train Loss: 0.764 | Acc: 73.115% (24052/32896)\n",
      "train Loss: 0.762 | Acc: 73.056% (36002/49280)\n",
      "test Loss: 0.773 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 151\n",
      "train Loss: 0.663 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.755 | Acc: 72.989% (12052/16512)\n",
      "train Loss: 0.761 | Acc: 72.869% (23971/32896)\n",
      "train Loss: 0.762 | Acc: 72.812% (35882/49280)\n",
      "test Loss: 0.840 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 152\n",
      "train Loss: 0.828 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.751 | Acc: 73.710% (12171/16512)\n",
      "train Loss: 0.756 | Acc: 73.462% (24166/32896)\n",
      "train Loss: 0.758 | Acc: 73.476% (36209/49280)\n",
      "test Loss: 0.821 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 153\n",
      "train Loss: 0.785 | Acc: 77.344% (99/128)\n",
      "train Loss: 0.747 | Acc: 73.492% (12135/16512)\n",
      "train Loss: 0.756 | Acc: 73.270% (24103/32896)\n",
      "train Loss: 0.757 | Acc: 73.257% (36101/49280)\n",
      "test Loss: 0.814 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 154\n",
      "train Loss: 0.790 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.749 | Acc: 73.716% (12172/16512)\n",
      "train Loss: 0.755 | Acc: 73.638% (24224/32896)\n",
      "train Loss: 0.754 | Acc: 73.626% (36283/49280)\n",
      "test Loss: 0.841 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 155\n",
      "train Loss: 0.672 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.760 | Acc: 73.413% (12122/16512)\n",
      "train Loss: 0.757 | Acc: 73.574% (24203/32896)\n",
      "train Loss: 0.757 | Acc: 73.502% (36222/49280)\n",
      "test Loss: 0.793 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 156\n",
      "train Loss: 0.728 | Acc: 76.562% (98/128)\n",
      "train Loss: 0.751 | Acc: 73.486% (12134/16512)\n",
      "train Loss: 0.748 | Acc: 73.687% (24240/32896)\n",
      "train Loss: 0.751 | Acc: 73.608% (36274/49280)\n",
      "test Loss: 0.784 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 157\n",
      "train Loss: 0.895 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.756 | Acc: 73.474% (12132/16512)\n",
      "train Loss: 0.755 | Acc: 73.580% (24205/32896)\n",
      "train Loss: 0.750 | Acc: 73.829% (36383/49280)\n",
      "test Loss: 0.774 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 158\n",
      "train Loss: 0.907 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.746 | Acc: 73.631% (12158/16512)\n",
      "train Loss: 0.744 | Acc: 73.903% (24311/32896)\n",
      "train Loss: 0.752 | Acc: 73.610% (36275/49280)\n",
      "test Loss: 0.785 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 159\n",
      "train Loss: 0.740 | Acc: 78.906% (101/128)\n",
      "train Loss: 0.757 | Acc: 73.444% (12127/16512)\n",
      "train Loss: 0.757 | Acc: 73.386% (24141/32896)\n",
      "train Loss: 0.751 | Acc: 73.529% (36235/49280)\n",
      "test Loss: 0.745 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 160\n",
      "train Loss: 0.703 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.762 | Acc: 73.383% (12117/16512)\n",
      "train Loss: 0.753 | Acc: 73.562% (24199/32896)\n",
      "train Loss: 0.752 | Acc: 73.697% (36318/49280)\n",
      "test Loss: 0.794 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 161\n",
      "train Loss: 0.782 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.747 | Acc: 73.686% (12167/16512)\n",
      "train Loss: 0.744 | Acc: 73.827% (24286/32896)\n",
      "train Loss: 0.748 | Acc: 73.782% (36360/49280)\n",
      "test Loss: 0.810 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 162\n",
      "train Loss: 0.873 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.742 | Acc: 73.795% (12185/16512)\n",
      "train Loss: 0.748 | Acc: 73.541% (24192/32896)\n",
      "train Loss: 0.752 | Acc: 73.476% (36209/49280)\n",
      "test Loss: 0.765 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 163\n",
      "train Loss: 0.671 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.755 | Acc: 73.674% (12165/16512)\n",
      "train Loss: 0.747 | Acc: 73.811% (24281/32896)\n",
      "train Loss: 0.750 | Acc: 73.803% (36370/49280)\n",
      "test Loss: 0.751 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 164\n",
      "train Loss: 0.762 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.742 | Acc: 73.989% (12217/16512)\n",
      "train Loss: 0.746 | Acc: 73.851% (24294/32896)\n",
      "train Loss: 0.745 | Acc: 73.991% (36463/49280)\n",
      "test Loss: 0.804 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 165\n",
      "train Loss: 0.763 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.740 | Acc: 73.783% (12183/16512)\n",
      "train Loss: 0.742 | Acc: 73.714% (24249/32896)\n",
      "train Loss: 0.745 | Acc: 73.612% (36276/49280)\n",
      "test Loss: 0.804 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 166\n",
      "train Loss: 0.725 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.736 | Acc: 73.807% (12187/16512)\n",
      "train Loss: 0.740 | Acc: 73.656% (24230/32896)\n",
      "train Loss: 0.744 | Acc: 73.718% (36328/49280)\n",
      "test Loss: 0.806 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 167\n",
      "train Loss: 0.853 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.741 | Acc: 73.910% (12204/16512)\n",
      "train Loss: 0.739 | Acc: 73.954% (24328/32896)\n",
      "train Loss: 0.745 | Acc: 73.746% (36342/49280)\n",
      "test Loss: 0.765 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 168\n",
      "train Loss: 0.583 | Acc: 82.031% (105/128)\n",
      "train Loss: 0.742 | Acc: 73.746% (12177/16512)\n",
      "train Loss: 0.744 | Acc: 73.665% (24233/32896)\n",
      "train Loss: 0.743 | Acc: 73.760% (36349/49280)\n",
      "test Loss: 0.812 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 169\n",
      "train Loss: 0.813 | Acc: 71.875% (92/128)\n",
      "train Loss: 0.735 | Acc: 74.340% (12275/16512)\n",
      "train Loss: 0.747 | Acc: 73.711% (24248/32896)\n",
      "train Loss: 0.745 | Acc: 73.689% (36314/49280)\n",
      "test Loss: 0.801 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 170\n",
      "train Loss: 0.820 | Acc: 68.750% (88/128)\n",
      "train Loss: 0.742 | Acc: 73.940% (12209/16512)\n",
      "train Loss: 0.745 | Acc: 73.811% (24281/32896)\n",
      "train Loss: 0.740 | Acc: 73.979% (36457/49280)\n",
      "test Loss: 0.757 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 171\n",
      "train Loss: 0.805 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.734 | Acc: 73.849% (12194/16512)\n",
      "train Loss: 0.735 | Acc: 73.957% (24329/32896)\n",
      "train Loss: 0.736 | Acc: 74.056% (36495/49280)\n",
      "test Loss: 0.776 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 172\n",
      "train Loss: 0.632 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.724 | Acc: 74.727% (12339/16512)\n",
      "train Loss: 0.740 | Acc: 73.915% (24315/32896)\n",
      "train Loss: 0.742 | Acc: 73.845% (36391/49280)\n",
      "test Loss: 0.806 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 173\n",
      "train Loss: 0.614 | Acc: 77.344% (99/128)\n",
      "train Loss: 0.743 | Acc: 73.789% (12184/16512)\n",
      "train Loss: 0.741 | Acc: 73.817% (24283/32896)\n",
      "train Loss: 0.740 | Acc: 73.817% (36377/49280)\n",
      "test Loss: 0.808 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 174\n",
      "train Loss: 0.551 | Acc: 80.469% (103/128)\n",
      "train Loss: 0.735 | Acc: 74.316% (12271/16512)\n",
      "train Loss: 0.732 | Acc: 74.359% (24461/32896)\n",
      "train Loss: 0.735 | Acc: 74.330% (36630/49280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.754 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 175\n",
      "train Loss: 0.661 | Acc: 79.688% (102/128)\n",
      "train Loss: 0.741 | Acc: 73.855% (12195/16512)\n",
      "train Loss: 0.736 | Acc: 74.182% (24403/32896)\n",
      "train Loss: 0.738 | Acc: 74.131% (36532/49280)\n",
      "test Loss: 0.785 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 176\n",
      "train Loss: 0.637 | Acc: 82.031% (105/128)\n",
      "train Loss: 0.727 | Acc: 74.715% (12337/16512)\n",
      "train Loss: 0.733 | Acc: 74.273% (24433/32896)\n",
      "train Loss: 0.734 | Acc: 74.203% (36567/49280)\n",
      "test Loss: 0.773 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 177\n",
      "train Loss: 0.621 | Acc: 76.562% (98/128)\n",
      "train Loss: 0.729 | Acc: 74.237% (12258/16512)\n",
      "train Loss: 0.729 | Acc: 74.273% (24433/32896)\n",
      "train Loss: 0.733 | Acc: 74.156% (36544/49280)\n",
      "test Loss: 0.797 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 178\n",
      "train Loss: 0.729 | Acc: 73.438% (94/128)\n",
      "train Loss: 0.731 | Acc: 74.358% (12278/16512)\n",
      "train Loss: 0.730 | Acc: 74.191% (24406/32896)\n",
      "train Loss: 0.731 | Acc: 74.249% (36590/49280)\n",
      "test Loss: 0.766 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 179\n",
      "train Loss: 0.693 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.724 | Acc: 74.400% (12285/16512)\n",
      "train Loss: 0.729 | Acc: 74.207% (24411/32896)\n",
      "train Loss: 0.730 | Acc: 74.237% (36584/49280)\n",
      "test Loss: 0.803 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 180\n",
      "train Loss: 0.764 | Acc: 70.312% (90/128)\n",
      "train Loss: 0.739 | Acc: 73.740% (12176/16512)\n",
      "train Loss: 0.739 | Acc: 73.754% (24262/32896)\n",
      "train Loss: 0.733 | Acc: 74.160% (36546/49280)\n",
      "test Loss: 0.790 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 181\n",
      "train Loss: 0.709 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.727 | Acc: 74.582% (12315/16512)\n",
      "train Loss: 0.734 | Acc: 74.313% (24446/32896)\n",
      "train Loss: 0.736 | Acc: 74.267% (36599/49280)\n",
      "test Loss: 0.773 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 182\n",
      "train Loss: 0.677 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.726 | Acc: 74.740% (12341/16512)\n",
      "train Loss: 0.725 | Acc: 74.672% (24564/32896)\n",
      "train Loss: 0.730 | Acc: 74.389% (36659/49280)\n",
      "test Loss: 0.807 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 183\n",
      "train Loss: 0.700 | Acc: 72.656% (93/128)\n",
      "train Loss: 0.734 | Acc: 74.322% (12272/16512)\n",
      "train Loss: 0.726 | Acc: 74.717% (24579/32896)\n",
      "train Loss: 0.728 | Acc: 74.537% (36732/49280)\n",
      "test Loss: 0.763 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 184\n",
      "train Loss: 0.860 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.734 | Acc: 74.437% (12291/16512)\n",
      "train Loss: 0.729 | Acc: 74.553% (24525/32896)\n",
      "train Loss: 0.732 | Acc: 74.416% (36672/49280)\n",
      "test Loss: 0.793 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 185\n",
      "train Loss: 0.668 | Acc: 77.344% (99/128)\n",
      "train Loss: 0.732 | Acc: 74.164% (12246/16512)\n",
      "train Loss: 0.725 | Acc: 74.447% (24490/32896)\n",
      "train Loss: 0.728 | Acc: 74.345% (36637/49280)\n",
      "test Loss: 0.773 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 186\n",
      "train Loss: 0.835 | Acc: 67.188% (86/128)\n",
      "train Loss: 0.731 | Acc: 74.158% (12245/16512)\n",
      "train Loss: 0.730 | Acc: 74.261% (24429/32896)\n",
      "train Loss: 0.731 | Acc: 74.166% (36549/49280)\n",
      "test Loss: 0.798 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 187\n",
      "train Loss: 0.649 | Acc: 78.125% (100/128)\n",
      "train Loss: 0.707 | Acc: 75.042% (12391/16512)\n",
      "train Loss: 0.715 | Acc: 74.827% (24615/32896)\n",
      "train Loss: 0.723 | Acc: 74.570% (36748/49280)\n",
      "test Loss: 0.794 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 188\n",
      "train Loss: 0.619 | Acc: 78.125% (100/128)\n",
      "train Loss: 0.715 | Acc: 74.964% (12378/16512)\n",
      "train Loss: 0.721 | Acc: 74.784% (24601/32896)\n",
      "train Loss: 0.722 | Acc: 74.706% (36815/49280)\n",
      "test Loss: 0.771 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 189\n",
      "train Loss: 0.690 | Acc: 75.000% (96/128)\n",
      "train Loss: 0.724 | Acc: 74.552% (12310/16512)\n",
      "train Loss: 0.722 | Acc: 74.699% (24573/32896)\n",
      "train Loss: 0.726 | Acc: 74.602% (36764/49280)\n",
      "test Loss: 0.788 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 190\n",
      "train Loss: 0.617 | Acc: 81.250% (104/128)\n",
      "train Loss: 0.718 | Acc: 74.843% (12358/16512)\n",
      "train Loss: 0.730 | Acc: 74.577% (24533/32896)\n",
      "train Loss: 0.729 | Acc: 74.533% (36730/49280)\n",
      "test Loss: 0.769 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 191\n",
      "train Loss: 0.760 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.726 | Acc: 74.612% (12320/16512)\n",
      "train Loss: 0.717 | Acc: 74.936% (24651/32896)\n",
      "train Loss: 0.722 | Acc: 74.645% (36785/49280)\n",
      "test Loss: 0.769 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 192\n",
      "train Loss: 0.756 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.727 | Acc: 74.709% (12336/16512)\n",
      "train Loss: 0.725 | Acc: 74.644% (24555/32896)\n",
      "train Loss: 0.727 | Acc: 74.489% (36708/49280)\n",
      "test Loss: 0.735 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 193\n",
      "train Loss: 0.644 | Acc: 82.031% (105/128)\n",
      "train Loss: 0.725 | Acc: 74.546% (12309/16512)\n",
      "train Loss: 0.720 | Acc: 74.653% (24558/32896)\n",
      "train Loss: 0.722 | Acc: 74.625% (36775/49280)\n",
      "test Loss: 0.801 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 194\n",
      "train Loss: 0.671 | Acc: 78.125% (100/128)\n",
      "train Loss: 0.728 | Acc: 74.170% (12247/16512)\n",
      "train Loss: 0.722 | Acc: 74.599% (24540/32896)\n",
      "train Loss: 0.720 | Acc: 74.692% (36808/49280)\n",
      "test Loss: 0.772 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 195\n",
      "train Loss: 0.823 | Acc: 71.094% (91/128)\n",
      "train Loss: 0.724 | Acc: 74.297% (12268/16512)\n",
      "train Loss: 0.720 | Acc: 74.511% (24511/32896)\n",
      "train Loss: 0.724 | Acc: 74.357% (36643/49280)\n",
      "test Loss: 0.763 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 196\n",
      "train Loss: 0.742 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.723 | Acc: 74.558% (12311/16512)\n",
      "train Loss: 0.720 | Acc: 74.766% (24595/32896)\n",
      "train Loss: 0.720 | Acc: 74.740% (36832/49280)\n",
      "test Loss: 0.823 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 197\n",
      "train Loss: 0.723 | Acc: 74.219% (95/128)\n",
      "train Loss: 0.723 | Acc: 75.042% (12391/16512)\n",
      "train Loss: 0.724 | Acc: 74.933% (24650/32896)\n",
      "train Loss: 0.721 | Acc: 74.901% (36911/49280)\n",
      "test Loss: 0.805 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 198\n",
      "train Loss: 0.841 | Acc: 69.531% (89/128)\n",
      "train Loss: 0.724 | Acc: 74.788% (12349/16512)\n",
      "train Loss: 0.722 | Acc: 74.732% (24584/32896)\n",
      "train Loss: 0.722 | Acc: 74.623% (36774/49280)\n",
      "test Loss: 0.774 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 199\n",
      "train Loss: 0.692 | Acc: 75.781% (97/128)\n",
      "train Loss: 0.725 | Acc: 74.419% (12288/16512)\n",
      "train Loss: 0.715 | Acc: 74.960% (24659/32896)\n",
      "train Loss: 0.713 | Acc: 74.963% (36942/49280)\n",
      "test Loss: 0.768 | Acc: 71.000% (71/100)\n",
      "3105.835368156433\n"
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "conv_op = D.DeformConv2d\n",
    "class DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNet, self).__init__()\n",
    "        self.conv1 = conv_op(3, 6, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = conv_op(6, 16, 3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "dnet = DNet().cuda()\n",
    "a = time()\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch, dnet)\n",
    "    test(epoch, dnet)\n",
    "print(time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
