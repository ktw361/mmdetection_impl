{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils import model_zoo\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import mmcv\n",
    "def gt_as_det(img_prefix, ann_dir='annotations', img_dir='images'):\n",
    "    \"\"\" What we need is list of dict with:\n",
    "        {'score', 'bbox', 'image_id', 'category_id'}\n",
    "    \"\"\"\n",
    "    all_images = osp.join(img_prefix, img_dir)\n",
    "    all_annos = osp.join(img_prefix, ann_dir)\n",
    "\n",
    "    detections = []\n",
    "    for img_ind, img_name in enumerate(os.listdir(all_images)):\n",
    "        stem = img_name.split('.')[0]\n",
    "        ann_name = stem + '.txt'\n",
    "        img_full_path = osp.join(all_images, img_name)\n",
    "        ann_full_path = osp.join(all_annos, ann_name)\n",
    "\n",
    "        image_id = int(hashlib.sha256(stem.encode('utf8')).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "        with open(ann_full_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [v.strip('\\n') for v in lines]\n",
    "            lines = [v.split(',') for v in lines]\n",
    "            lines = np.asarray(lines)[:, :8].astype(np.int32)\n",
    "\n",
    "        for line in lines:\n",
    "            x1, y1, w, h, sc, label, trun, occ = line\n",
    "            label = int(label)\n",
    "            if label == 0 or label == 11:\n",
    "                # ignore ignore(0) and others(11)\n",
    "                continue\n",
    "            assert label > 0 and label < 11, 'Bad annotation label'\n",
    "            x2, y2 = x1 + w, y1 + h\n",
    "            # label is number, '0' is background\n",
    "            bbox = np.asarray([x1, y1, x2, y2]).tolist()\n",
    "            id = str(stem) + str(bbox)\n",
    "            id = int(hashlib.sha256(id.encode('utf8')).hexdigest(), 16) % (10 ** 12)\n",
    "\n",
    "            _det = dict(\n",
    "                image_id=image_id,\n",
    "                bbox=bbox,\n",
    "                category_id=label,\n",
    "                score=1,\n",
    "            )\n",
    "            detections.append(_det)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = gt_as_det('/home/damon/DATASETS/Drone2019/VisDrone2019-DET/VisDrone2018-DET-val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = '/tmp/gtdt.json'\n",
    "mmcv.dump(detections, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now eval on coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cocoGt = COCO('/home/damon/DATASETS/Drone2019/VisDrone2019-DET/VisDrone2018-DET-val/annotations_val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cocoDt = cocoGt.loadRes(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIds = sorted(cocoGt.getImgIds())\n",
    "# imgIds = imgIds[0:100]\n",
    "# imgId = imgIds[np.random.randint(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=22.60s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.38s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.996\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.996\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.996\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.997\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.997\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.997\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.779\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.998\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.998\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n"
     ]
    }
   ],
   "source": [
    "annType = 'bbox'\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, annType)\n",
    "cocoEval.params.imgIds = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8262e3005208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
